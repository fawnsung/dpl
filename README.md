# DPL - Data-Parallel LLM Inference and Resource Monitoring Platform

A multi-node, local/on-prem LLM inference orchestration and monitoring platform:
- Frontend Web UI to visualize cluster resources (CPU/GPU/Memory/Network/Disk) and provide a chat and dataset batch-processing interface
- Backend Gateway that proxies requests to upstream LLMs (Ollama/OpenAI-compatible), performs load-aware parallel scheduling, and supports node lock/unlock
- Per-node Monitor Agent that collects machine metrics and provides lock management APIs for Gateway routing and preemption
- A health-check script to quickly inspect Tailscale connectivity and node availability


## Feature Overview

- Cluster overview and single-host details: CPU utilization, memory, NIC throughput, disk throughput, GPU utilization/memory/temperature/power (NVIDIA), with basic detection on Apple Silicon
- Multi-source LLM proxy: prefer Ollama `/api/chat` and auto-fallback to `/api/generate` when needed; supports OpenAI-style `/v1/chat/completions`
- Load-aware scheduling: prioritize online, unlocked nodes that match the model; allocate by lowest load preferring GPU â†’ CPU; node locks are auto-released at the end of sessions/tasks with safety fallbacks
- Model management: list/show running models and trigger on-demand warmup (for Ollama, warm via `/api/generate`)
- Dataset batch processing: upload a JSON array and process it in parallel across multiple nodes; real-time progress and stats; graceful stop and result download
- System alerts: threshold-based alerts on CPU/Memory/GPU metrics
- Health-check script: one-command sweep of Tailscale, node Monitor APIs, and LLM API availability


## Project Structure

- `frontend/`: static web frontend (`index.html`, `script.js`), statically served by the `gateway`
- `gateway/`: core FastAPI gateway with load balancing, SSE forwarding, dataset task orchestration, and alert logic (`gateway.py`)
- `monitor_agent/`: per-node FastAPI monitoring agent (`agent.py`) that collects hardware metrics and provides lock/unlock APIs
- `scripts/`: ops scripts (e.g., `health_check.py`)
- `alpaca_data.json`: sample/test data (large file)
- `requirements.txt`: Python dependencies
- `README.md`: this document


## Prerequisites

- Python 3.10+
- Recommended: NVIDIA driver and CUDA (for detailed GPU metrics), or Apple Silicon (basic detection)
- Recommended: Tailscale (or other LAN/VPN) for multi-node connectivity
- Local LLM service on each node:
  - Prefer Ollama (default `http://127.0.0.1:11434`) or other OpenAI-compatible endpoints


## Install Dependencies

```bash
pip install -r requirements.txt
```

(For Windows PowerShell, using a virtual environment is recommended.)


## Configuration

All gateway-side node topology is defined in `gateway/gateway.py` under `Settings.NODES`:

```python
class Settings:
    GATEWAY_PORT = 8000
    NODES = [
        {
            "id": 1, "name": "Node 1 (Host 1)",
            "monitor_base_url": "http://127.0.0.1:8001",
            "llm_url": "http://127.0.0.1:11434/api/chat",
        },
        # ... more nodes ...
    ]
```

- Update each node's monitoring agent address (`monitor_base_url`) and LLM endpoint (`llm_url`) to reachable addresses (via Tailscale/SSH tunnel, etc.).
- If using Ollama, `llm_url` is recommended as `/api/chat`; the gateway will auto-fallback to `/api/generate` on 404/405/501.
- The gateway listens on port `8000` by default; adjust `GATEWAY_PORT` if needed.

On each node (`monitor_agent/agent.py`), you can set defaults via environment variables or `.env`:
- `OLLAMA_MODEL`: preferred model name (otherwise the agent will auto-select via `/api/tags`)
- `OLLAMA_HOST`: default `http://127.0.0.1:11434`
- `PORT`: monitor agent listen port (default `8001`)


## Run and Access

1) Start the monitor agent on each node:
```bash
cd monitor_agent
python agent.py
```

2) Start the gateway (from project root):
```bash
python -m uvicorn gateway.gateway:app --reload --host 0.0.0.0 --port 8000
```

3) Open the frontend:
- Visit `http://localhost:8000/` in your browser
- Static pages under `frontend/` are served by default


## Frontend Usage Guide

- Hosts Overview: shows per-host online/offline, CPU/Memory/GPU key metrics, and running models
- Host Details: single-host real-time charts (CPU, NIC Mbps, Disk MB/s, GPU temperature/power/utilization)
- Chat: choose a model (or auto) and chat with the LLM via streaming; the page shows the assigned node first, then streams tokens
- Dataset Processing:
  - Upload a JSON file (array) where each item has `instruction` and optional `input`
  - Choose how many items to process; after creating a job, view overall progress, per-node stats, and error details
  - Download merged results with `model_output` after completion
- Alerts: show active alerts periodically generated by the gateway


## Primary APIs (Gateway)

- Node Status and Alerts
  - `GET /api/status/all`: get cached node status (online/locked/metrics)
  - `GET /api/alerts`: get current active alerts
  - `GET /api/cluster/overview`: cluster-level aggregated metrics (for frontend multi-host charts)

- Model Management and Sessions
  - `GET /api/models`: list available/running models (merge node reports with Ollama `/api/tags` as fallback)
  - `POST /api/models/start/{model}`: warm up a specified model on an available node
  - `POST /api/chat/completions`: unified chat proxy (auto-select best node and forward; returns SSE stream)
  - `POST /api/unlock/all`: emergency unlock of all nodes (when locks get stuck)

- Dataset Batch Processing (async)
  - `POST /api/dataset/upload`: upload a JSON file and create a job; returns `job_id` immediately
  - `GET /api/dataset/status/{job_id}`: brief job progress
  - `GET /api/dataset/detailed-status/{job_id}`: detailed stats (success/failure/pending, per-node stats, durations, etc.)
  - `POST /api/dataset/stop/{job_id}`: gracefully stop the job (wait for nodes to finish the current item)
  - `GET /api/dataset/result/{job_id}`: download results of a completed or stopped job
  - `POST /api/dataset/cleanup/all`: forcibly clean up stuck jobs and attempt to unlock all nodes

- Diagnostics and Prometheus Proxy
  - `GET /api/debug/info`, `GET /api/debug/diagnosis`: runtime diagnostics
  - `GET /api/prometheus/query_range`: Prometheus query proxy (`PROMETHEUS_URL` can be changed in `gateway.py`)


## Monitor Agent (Per Node)

- `GET /status`: returns lock state, CPU/Memory/GPU metrics, current model, and network/disk throughput (computed via sampled snapshots)
- `POST /lock`, `POST /unlock`: node lock management (used by the gateway during session/task lifecycles)
- GPU metrics:
  - NVIDIA: via `pynvml` for utilization/memory/temperature/power
  - macOS: attempt lightweight detection via `powermetrics` and PyTorch MPS availability


## Health-Check Script

`scripts/health_check.py` can inspect:
- Tailscale runtime status
- Node monitor APIs and LLM API availability

Examples:
```bash
python scripts/health_check.py           # single run
python scripts/health_check.py -c -i 60  # continuous run every 60 seconds
```

Note: adjust `NODES` IP/ports in the script to match your environment.


## Dataset Format Example

Uploaded JSON must be an array; each element example:

```json
{
  "instruction": "Write a short introduction for the title below",
  "input": "Design of a Data-Parallel Inference Platform"
}
```

The downloaded results will be an array one-to-one with the originals, with an added `model_output` field per item (for success, it is the upstream return object; for failure, it is `{ "error": "..." }`).


## FAQ

- Frontend shows all hosts offline?
  - Check that the gateway is running and listening on `8000`; verify `monitor_base_url` in `Settings.NODES` is reachable; ensure each node's monitor agent is running.

- Specified model not found?
  - The gateway will try to warm up the model on an available node via `/api/generate`. Make sure the node's Ollama has pulled the model image, or has network access to pull it automatically.

- Node remains locked?
  - The gateway unlocks on stream end or task teardown; if `[DONE]` is lost, it unlocks in `finally` as a fallback. If it still misbehaves, call `POST /api/unlock/all`.

- GPU metrics are empty?
  - NVIDIA: ensure drivers are installed and accessible by `pynvml`; on macOS only limited detection is provided.


## Security and Deployment Recommendations

- In production, strictly limit the CORS whitelist and the domain serving the frontend
- Place the gateway and monitor agents on a controlled internal network, accessed via Tailscale/VPN
- If exposed externally, consider a reverse proxy and authentication (e.g., OAuth/Token) in front of the gateway


## Roadmap (Optional)

- More advanced scheduling (historical latency, concurrency, adaptive weighting)
- Multi-replica inference and consistency merging
- Richer alert rules and notification channels (Webhook/Email)
- Enhanced frontend visualizations and multi-cluster views


## License

Third-party components are distributed under their respective licenses. Please add a license for this project as needed.


# dpl
